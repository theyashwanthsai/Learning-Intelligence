# Plan for learning Deep Learning

#### Pre Req
- [x] Daniel Bourke's Course on Pytorch. [Youtube Link](https://youtu.be/Z_ikDlimN6A?si=U2Cjn_yjpvfmvFlV)
- [x] Pick one dataset and do a clean ML workflow on it

### ML Breakthroughs Chronological Implementations

Implementing the most important ML research papers using PyTorch.  
Goal: Read, understand, and implement each paper to deeply learn architectures & training methods.

---

## âœ… Progress Checklist

### Phase 1 â€“ Foundations (1958â€“2012)
- [x] **1958** â€“ Perceptron ([Paper](https://en.wikipedia.org/wiki/Perceptron))
- [x] **1986** â€“ Backpropagation MLP ([Paper](https://www.nature.com/articles/323533a0))
- [x] **1998** â€“ LeNet-5 ([Paper](http://yann.lecun.com/exdb/lenet/))
- [x] **2012** â€“ AlexNet ([Paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf))

### Phase 2 â€“ CNN Revolution (2014â€“2016)
- [ ] **2014** â€“ VGGNet ([Paper](https://arxiv.org/abs/1409.1556))
- [ ] **2014** â€“ GoogLeNet / Inception v1 ([Paper](https://arxiv.org/abs/1409.4842))
- [ ] **2015** â€“ ResNet ([Paper](https://arxiv.org/abs/1512.03385))
- [ ] **2016** â€“ DenseNet ([Paper](https://arxiv.org/abs/1608.06993))

### Phase 3 â€“ Sequence Models & Attention (2013â€“2018)
- [ ] **2013** â€“ Word2Vec ([Paper](https://arxiv.org/abs/1301.3781))
- [ ] **2014** â€“ Seq2Seq + Attention ([Paper](https://arxiv.org/abs/1409.0473))
- [ ] **2017** â€“ Transformer ([Paper](https://arxiv.org/abs/1706.03762))
- [ ] **2018** â€“ BERT ([Paper](https://arxiv.org/abs/1810.04805))

### Phase 4 â€“ Modern Vision & Multimodal (2020â€“2022)
- [ ] **2020** â€“ Vision Transformer ([Paper](https://arxiv.org/abs/2010.11929))
- [ ] **2021** â€“ CLIP ([Paper](https://arxiv.org/abs/2103.00020))
- [ ] **2021** â€“ Swin Transformer ([Paper](https://arxiv.org/abs/2103.14030))

### Phase 5 â€“ Generative Models (2014â€“2023)
- [ ] **2014** â€“ GAN ([Paper](https://arxiv.org/abs/1406.2661))
- [ ] **2015** â€“ DCGAN ([Paper](https://arxiv.org/abs/1511.06434))
- [ ] **2017** â€“ Pix2Pix ([Paper](https://arxiv.org/abs/1611.07004))
- [ ] **2020** â€“ DDPM (Diffusion Models) ([Paper](https://arxiv.org/abs/2006.11239))

### Phase 6 â€“ Reinforcement Learning (2013â€“2023)
- [ ] **2013** â€“ DQN ([Paper](https://arxiv.org/abs/1312.5602))
- [ ] **2016** â€“ A3C ([Paper](https://arxiv.org/abs/1602.01783))
- [ ] **2018** â€“ AlphaZero (Mini) ([Paper](https://arxiv.org/abs/1712.01815))

---
If you follow this, youâ€™ll be dangerously good at PyTorch in ~4â€“6 months and youâ€™ll also be fluent in reading & implementing papers, which is a big research skill.

## ðŸ›  How I Work on Each Paper
1. Read abstract & intro.  
2. Understand architecture diagram.  
3. Implement minimal version in PyTorch.  
4. Test on small dataset.  
5. Document learnings in a short note.  

---

## ðŸŽ¯ Goal
- Become fluent in PyTorch by replicating historical breakthroughs.  
- Build a public portfolio of research implementations.  
- Learn to read & implement papers quickly.




